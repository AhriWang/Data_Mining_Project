import pandas as pd
import numpy as np
import re
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk
import os
import swifter  # âœ… ç”¨äºå¹¶è¡ŒåŠ é€Ÿ apply()

# === ä¸‹è½½å¹¶é…ç½® NLTK åœç”¨è¯ ===
nltk.data.path.append('/root/nltk_data')
nltk.download('stopwords', force=True)

# âœ… å°†åœç”¨è¯è®¾ç½®ä¸ºå…¨å±€å˜é‡ï¼Œå‡å°‘é‡å¤åŠ è½½
STOP_WORDS = set(stopwords.words('english'))

# === è¯»å– CSV æ–‡ä»¶å¹¶è®¾ç½®åˆ—å ===
train_df1 = pd.read_csv('/root/datamining/train_part_1.csv', names=['label', 'review_title', 'review_text'], header=None)
train_df2 = pd.read_csv('/root/datamining/train_part_2.csv', names=['label', 'review_title', 'review_text'], header=None)
test_df = pd.read_csv('/root/datamining/test.csv', names=['label', 'review_title', 'review_text'], header=None)

# === åˆå¹¶æ•°æ®é›† ===
df = pd.concat([train_df1, train_df2, test_df], ignore_index=True)

# === åˆå¹¶æ ‡é¢˜å’Œæ–‡æœ¬åˆ— ===
df['text'] = df['review_title'].astype(str) + ' ' + df['review_text'].astype(str)

# âœ… å®šä¹‰é¢„å¤„ç†å‡½æ•°ï¼ˆç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿ä»£ word_tokenizeï¼‰
def preprocess_text(text):
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s]', '', text)
    # åˆ†è¯ï¼ˆç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿ä»£ word_tokenizeï¼‰
    tokens = re.findall(r'\w+', text)
    # å»é™¤åœç”¨è¯
    tokens = [word for word in tokens if word not in STOP_WORDS]
    return tokens

# âœ… ä½¿ç”¨ swifter è¿›è¡Œå¹¶è¡ŒåŠ é€Ÿ apply()
print("\nğŸš€ æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬æ•°æ®ï¼ˆä½¿ç”¨ swifter åŠ é€Ÿï¼‰...")
df['processed_text'] = df['text'].swifter.apply(preprocess_text)

# === ä½¿ç”¨ TfidfVectorizer è¿›è¡Œåˆ†è¯ ===
vectorizer = TfidfVectorizer(
    tokenizer=lambda x: x,    # ç›´æ¥ä½¿ç”¨åˆ†è¯åçš„ç»“æœ
    lowercase=False,          # å·²ç»å°å†™åŒ–
    token_pattern=None        # å¿½ç•¥é»˜è®¤çš„æ­£åˆ™åŒ¹é…
)
X = vectorizer.fit_transform(df['processed_text'])

# === å°†åˆ†è¯ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¾› Word2Vec ä½¿ç”¨ ===
sentences = df['processed_text'].tolist()

# âœ… æ„å»º Word2Vec æ¨¡å‹ï¼ˆåˆ©ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼‰
print("\nğŸš€ æ­£åœ¨è®­ç»ƒ Word2Vec æ¨¡å‹...")
model = Word2Vec(
    sentences=sentences,      # ä½¿ç”¨åˆ†è¯åçš„ç»“æœ
    vector_size=100,          # è¯å‘é‡ç»´åº¦
    window=5,                 # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=2,              # æœ€ä½å‡ºç°æ¬¡æ•°
    workers=os.cpu_count(),   # ä½¿ç”¨ CPU æœ€å¤§çº¿ç¨‹æ•°
    epochs=20                 # è®­ç»ƒè½®æ•°
)

# âœ… è®­ç»ƒæ¨¡å‹ï¼ˆå¯ä»¥é€‚å½“å¢å¤§ epochs ä»¥æå‡æ•ˆæœï¼‰
model.train(sentences, total_examples=len(sentences), epochs=20)

# === ä¿å­˜æ¨¡å‹ ===
model.save('/root/datamining/amazon_word2vec.model')
print("\nâœ… Word2Vec æ¨¡å‹å·²ä¿å­˜ï¼")

# === åŠ è½½æ¨¡å‹ ===
model = Word2Vec.load('/root/datamining/amazon_word2vec.model')
print("\nâœ… Word2Vec æ¨¡å‹å·²åŠ è½½ï¼")

# === ç¤ºä¾‹1ï¼šè¾“å‡ºæŸä¸ªè¯çš„è¯å‘é‡ ===
word = 'good'
if word in model.wv:
    print(f"\nVector for '{word}':")
    print(model.wv[word])
else:
    print(f"\n'{word}' not in vocabulary")

# === ç¤ºä¾‹2ï¼šæŸ¥çœ‹ä¸æŸä¸ªè¯æœ€ç›¸ä¼¼çš„è¯ ===
if word in model.wv:
    print(f"\nMost similar words to '{word}':")
    print(model.wv.most_similar(word))

# === ç¤ºä¾‹3ï¼šè·å–è¯æ±‡è¡¨å¤§å° ===
print("\nVocabulary size:")
print(len(model.wv.index_to_key))
