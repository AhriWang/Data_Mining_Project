import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from tqdm import tqdm

# === è®¾ç½® tqdm çš„ pandas æ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
tqdm.pandas()

# === æ­¥éª¤ 1ï¼šåŠ è½½æ•°æ® ===
INPUT_CSV = "cleaned_pubmed_data.csv"
df = pd.read_csv(INPUT_CSV)
df = df.dropna(subset=["abstract"]).reset_index(drop=True)

# === æ­¥éª¤ 2ï¼šåŠ è½½é¢„è®­ç»ƒçš„åŒ»å­¦ NER æ¨¡å‹ï¼ˆBioBERTï¼‰===
model_name = "d4data/biobert-chemical-disease-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# === æ­¥éª¤ 3ï¼šæ„å»º transformers pipeline ===
nlp_ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

# === æ­¥éª¤ 4ï¼šå®šä¹‰å®ä½“æå–å‡½æ•° ===
def extract_entities(text):
    try:
        text = str(text)
        truncated = text[:512]  # BERT æœ€å¤§é•¿åº¦é™åˆ¶
        results = nlp_ner(truncated)
        entities = [(ent['word'], ent['entity_group']) for ent in results]
        return entities
    except Exception as e:
        return []

# === æ­¥éª¤ 5ï¼šæå–å®ä½“å¹¶æ·»åŠ åˆ°æ–°åˆ— ===
print("ğŸ” æ­£åœ¨æå–å®ä½“ï¼Œè¯·ç¨ç­‰...")
df["entities"] = df.progress_apply(lambda row: extract_entities(f"{row['title']}. {row['abstract']}"), axis=1)

# === æ­¥éª¤ 6ï¼šä¿å­˜ç»“æœåˆ°æ–°æ–‡ä»¶ ===
OUTPUT_CSV = "pubmed_ner_result.csv"
df.to_csv(OUTPUT_CSV, index=False)
print(f"âœ… æå–å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {OUTPUT_CSV}ï¼Œå…± {len(df)} æ¡è®°å½•")
